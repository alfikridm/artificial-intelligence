{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guido@python.org', 'guido@google.com']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "email = re.compile('\\w+@\\w+\\.[a-z]{3}')\n",
    "text = 'To email Guido, try guido@python.org or the older address guido@google.com'\n",
    "email.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mask = email.sub('_email_', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To email Guido, try _email_ or the older address _email_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t2\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "text = ['The quick brown fox jumped over the lazy dog']\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "vector = vectorizer.transform(text)\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n",
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "vector2 = vectorizer2.fit_transform(text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "print(vectorizer2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text   t1   t2   t3\n",
       "0   ham  Go until jurong point, crazy.. Available only ...  NaN  NaN  NaN\n",
       "1   ham                      Ok lar... Joking wif u oni...  NaN  NaN  NaN\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...  NaN  NaN  NaN\n",
       "3   ham  U dun say so early hor... U c already then say...  NaN  NaN  NaN\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...  NaN  NaN  NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['label', 'text', 't1', 't2', 't3']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  t1  t2  t3\n",
       "label                  \n",
       "ham    4825  45  10   6\n",
       "spam    747   5   2   0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "spam_count = data.groupby('label').count()\n",
    "spam_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Spam Texts')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(spam_count.index.values, spam_count['text'])\n",
    "plt.xlabel('Spam Label')\n",
    "plt.ylabel('Number of Spam Texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_table = data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_table = text_table.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in set(stop_words))\n",
    ")\n",
    "text_table = text_table.str.replace(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "                                   'emailaddr')\n",
    "text_table = text_table.str.replace(r'(https[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\s*)',\n",
    "                                   'httpaddr')\n",
    "text_table = text_table.str.replace(\n",
    "    r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "    'phonenumbr')\n",
    "text_table = text_table.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "text_table = text_table.str.lower()\n",
    "text_table = text_table.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "text_table = text_table.str.replace(r'\\s+',' ')\n",
    "text_table = text_table.str.replace(r'^\\s+|\\s+?$','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "data.text = text_table.apply(lambda x: ' '.join(\n",
    "    porter.stem(term) for term in x.split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x7758 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44881 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "text1=cv.fit_transform(text_table)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "text_table2 = text_table.apply(lambda x: ' '.join(\n",
    "    porter.stem(term) for term in x.split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x6464 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 45140 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=cv.fit_transform(text_table2)\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=' '.join(data.text)\n",
    "str_list = s.lower().split()\n",
    "unique_words = set(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictwords = dict()\n",
    "for words in unique_words:\n",
    "    dictwords[words] = [str_list.count(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataham = data[data.label == 'ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataspam = data[data.label == 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sham = ' '.join(dataham.text)\n",
    "sspam = ' '.join(dataspam.text)\n",
    "sham_list = sham.lower().split()\n",
    "sspam_list = sspam.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in unique_words :\n",
    "    iham=sham_list.count(words)\n",
    "    ispam=sspam_list.count(words)\n",
    "    dictwords[words].append(iham)\n",
    "    dictwords[words].append(ispam)\n",
    "    dictwords[words].append(ispam-iham)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortfreq = sorted(dictwords.items(), key=lambda x: x[1][3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbr', [2458, 871, 1587, 716]),\n",
       " ('phonenumbr', [407, 1, 406, 405]),\n",
       " ('Ã¥', [300, 4, 296, 292]),\n",
       " ('free', [282, 59, 223, 164]),\n",
       " ('txt', [186, 14, 172, 158]),\n",
       " ('numbrp', [151, 1, 150, 149]),\n",
       " ('mobil', [152, 14, 138, 124]),\n",
       " ('claim', [114, 0, 114, 114]),\n",
       " ('prize', [94, 0, 94, 94]),\n",
       " ('tone', [85, 0, 85, 85]),\n",
       " ('call', [660, 289, 371, 82]),\n",
       " ('stop', [166, 45, 121, 76]),\n",
       " ('uk', [73, 1, 72, 71]),\n",
       " ('servic', [78, 5, 73, 68]),\n",
       " ('nokia', [73, 3, 70, 67]),\n",
       " ('award', [67, 1, 66, 65]),\n",
       " ('cash', [89, 13, 76, 63]),\n",
       " ('repli', [162, 50, 112, 62]),\n",
       " ('to', [107, 24, 83, 59]),\n",
       " ('text', [229, 86, 143, 57]),\n",
       " ('urgent', [69, 6, 63, 57]),\n",
       " ('guarante', [50, 0, 50, 50]),\n",
       " ('contact', [75, 14, 61, 47]),\n",
       " ('win', [78, 17, 61, 44]),\n",
       " ('cs', [44, 0, 44, 44]),\n",
       " ('min', [115, 36, 79, 43]),\n",
       " ('collect', [60, 9, 51, 42]),\n",
       " ('custom', [62, 11, 51, 40]),\n",
       " ('numbrppm', [40, 0, 40, 40]),\n",
       " ('httpaddr', [113, 38, 75, 37]),\n",
       " ('rington', [37, 0, 37, 37]),\n",
       " ('voucher', [38, 1, 37, 36]),\n",
       " ('rate', [42, 3, 39, 36]),\n",
       " ('your', [135, 50, 85, 35]),\n",
       " ('draw', [45, 5, 40, 35]),\n",
       " ('per', [58, 12, 46, 34]),\n",
       " ('latest', [40, 3, 37, 34]),\n",
       " ('landlin', [36, 2, 34, 32]),\n",
       " ('camera', [38, 3, 35, 32]),\n",
       " ('offer', [49, 9, 40, 31]),\n",
       " ('video', [35, 2, 33, 31]),\n",
       " ('chat', [61, 15, 46, 31]),\n",
       " ('code', [31, 1, 30, 29]),\n",
       " ('won', [28, 0, 28, 28]),\n",
       " ('receiv', [46, 9, 37, 28]),\n",
       " ('line', [47, 10, 37, 27]),\n",
       " ('poli', [27, 0, 27, 27]),\n",
       " ('await', [28, 1, 27, 26]),\n",
       " ('select', [36, 5, 31, 26]),\n",
       " ('mob', [26, 0, 26, 26]),\n",
       " ('appli', [34, 4, 30, 26]),\n",
       " ('entri', [26, 0, 26, 26]),\n",
       " ('httpaddrhttpaddr', [30, 2, 28, 26]),\n",
       " ('po', [33, 4, 29, 25]),\n",
       " ('box', [35, 5, 30, 25]),\n",
       " ('wk', [40, 8, 32, 24]),\n",
       " ('weekli', [24, 0, 24, 24]),\n",
       " ('orang', [32, 4, 28, 24]),\n",
       " ('valid', [23, 0, 23, 23]),\n",
       " ('network', [34, 6, 28, 22]),\n",
       " ('nation', [22, 0, 22, 22]),\n",
       " ('bonu', [21, 0, 21, 21]),\n",
       " ('sae', [21, 0, 21, 21]),\n",
       " ('http', [21, 0, 21, 21]),\n",
       " ('deliveri', [22, 1, 21, 20]),\n",
       " ('attempt', [24, 2, 22, 20]),\n",
       " ('unsubscrib', [19, 0, 19, 19]),\n",
       " ('pound', [28, 5, 23, 18]),\n",
       " ('club', [20, 1, 19, 18]),\n",
       " ('privat', [19, 1, 18, 17]),\n",
       " ('cost', [41, 12, 29, 17]),\n",
       " ('chanc', [41, 12, 29, 17]),\n",
       " ('expir', [19, 1, 18, 17]),\n",
       " ('holiday', [47, 15, 32, 17]),\n",
       " ('pobox', [16, 0, 16, 16]),\n",
       " ('identifi', [16, 0, 16, 16]),\n",
       " ('mobileupdnumbr', [16, 0, 16, 16]),\n",
       " ('opt', [18, 1, 17, 16]),\n",
       " ('boxnumbr', [16, 0, 16, 16]),\n",
       " ('reveal', [18, 1, 17, 16]),\n",
       " ('music', [24, 4, 20, 16]),\n",
       " ('winner', [16, 0, 16, 16]),\n",
       " ('date', [42, 13, 29, 16]),\n",
       " ('ltd', [16, 0, 16, 16]),\n",
       " ('land', [20, 2, 18, 16]),\n",
       " ('numbrst', [41, 13, 28, 15]),\n",
       " ('www', [15, 0, 15, 15]),\n",
       " ('poboxnumbr', [15, 0, 15, 15]),\n",
       " ('numbrland', [15, 0, 15, 15]),\n",
       " ('camcord', [15, 0, 15, 15]),\n",
       " ('game', [41, 13, 28, 15]),\n",
       " ('suitenumbr', [15, 0, 15, 15]),\n",
       " ('pic', [38, 12, 26, 14]),\n",
       " ('freemsg', [14, 0, 14, 14]),\n",
       " ('oper', [16, 1, 15, 14]),\n",
       " ('statement', [18, 2, 16, 14]),\n",
       " ('wkli', [14, 0, 14, 14]),\n",
       " ('numbrhr', [28, 7, 21, 14]),\n",
       " ('numbrmth', [14, 0, 14, 14]),\n",
       " ('colour', [26, 6, 20, 14])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictfreq = sortfreq[:100]\n",
    "len(dictfreq)\n",
    "dictfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "text_counts=cv.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_counts, data['label'], test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.9838709677419355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(x_train, y_train)\n",
    "predicted=clf.predict(x_test)\n",
    "print('MultinomialNB Accuracy:',metrics.accuracy_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " climbing\n",
      " cat\n",
      " ninja\n",
      " eating\n",
      " impressed\n",
      " google\n",
      " feedback\n",
      " face\n",
      " extension\n",
      " ve\n",
      "Cluster 1:\n",
      " google\n",
      " best\n",
      " translate\n",
      " app\n",
      " feedback\n",
      " impressed\n",
      " map\n",
      " incredible\n",
      " ve\n",
      " taken\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "documents = [\"this litlle kitty came to play when I was eating at a restarant.\",\n",
    "            \"Merley. has the best squooshy kitten belly\",\n",
    "            \"Google Translate app is incredible.\",\n",
    "            \"If you open 100 tab in google you get a smiley face.\",\n",
    "            \"Best c at photo I've ever taken.\",\n",
    "            \"Climbing ninja cat.\",\n",
    "            \"Impressed with google map feedback.\",\n",
    "            \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "data[\"label_code\"] = lb_make.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_counts, data['label_code'], test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\asus\\Anaconda3a\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "input_dim = x_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                64650     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 64,661\n",
      "Trainable params: 64,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5240    0\n",
       "544     0\n",
       "2653    0\n",
       "1139    0\n",
       "2045    0\n",
       "1225    0\n",
       "2527    0\n",
       "4269    0\n",
       "3856    0\n",
       "3530    0\n",
       "513     0\n",
       "2684    0\n",
       "906     1\n",
       "3872    0\n",
       "4805    0\n",
       "4171    0\n",
       "2037    0\n",
       "1884    0\n",
       "3023    0\n",
       "62      0\n",
       "2075    0\n",
       "3725    0\n",
       "1785    0\n",
       "108     0\n",
       "517     1\n",
       "684     0\n",
       "778     0\n",
       "45      0\n",
       "2305    0\n",
       "3792    0\n",
       "       ..\n",
       "1031    0\n",
       "1110    0\n",
       "1888    0\n",
       "3550    0\n",
       "1527    0\n",
       "753     0\n",
       "3049    0\n",
       "2628    0\n",
       "562     0\n",
       "4764    0\n",
       "3562    1\n",
       "252     0\n",
       "2516    0\n",
       "2962    0\n",
       "4453    0\n",
       "5374    0\n",
       "5396    0\n",
       "1202    0\n",
       "3462    0\n",
       "2797    0\n",
       "4225    0\n",
       "144     0\n",
       "5056    0\n",
       "2895    1\n",
       "2763    0\n",
       "905     0\n",
       "5192    0\n",
       "3980    0\n",
       "235     0\n",
       "5157    0\n",
       "Name: label_code, Length: 5014, dtype: int32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\asus\\Anaconda3a\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4512 samples, validate on 502 samples\n",
      "Epoch 1/3\n",
      "4512/4512 [==============================] - 7s 1ms/step - loss: 0.4247 - acc: 0.9466 - val_loss: 0.2183 - val_acc: 0.9761\n",
      "Epoch 2/3\n",
      "4512/4512 [==============================] - 2s 456us/step - loss: 0.1507 - acc: 0.9829 - val_loss: 0.0995 - val_acc: 0.9861\n",
      "Epoch 3/3\n",
      "4512/4512 [==============================] - 2s 435us/step - loss: 0.0764 - acc: 0.9900 - val_loss: 0.0613 - val_acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9924\n",
      "Testing Accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(unique_words), 50))\n",
    "model2.add(Bidirectional(LSTM(32)))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4512 samples, validate on 502 samples\n",
      "Epoch 1/2\n",
      "3072/4512 [===================>..........] - ETA: 25:52 - loss: 0.4353 - acc: 0.8617"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, batch_size=32, epochs=2, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(x_train, y_train, verbose=False)\n",
    "print('Training Accuracy: {:.4f}'.format(accuracy))\n",
    "loss, accuracy = model2.evaluate(x_test, y_test, verbose=False)\n",
    "print('Testing Accuracy: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
